{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#熱重啟\n",
    "from keras.callbacks import Callback\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "class SGDRScheduler(Callback):\n",
    "\n",
    "    def __init__(self,\n",
    "                 min_lr,\n",
    "                 max_lr,\n",
    "                 steps_per_epoch,\n",
    "                 lr_decay=1,\n",
    "                 cycle_length=10,\n",
    "                 mult_factor=2):\n",
    "\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.lr_decay = lr_decay\n",
    "\n",
    "        self.batch_since_restart = 0\n",
    "        self.next_restart = cycle_length\n",
    "\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        self.cycle_length = cycle_length\n",
    "        self.mult_factor = mult_factor\n",
    "\n",
    "        self.history = {}\n",
    "\n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
    "        return lr\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        self.batch_since_restart += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
    "        if epoch + 1 == self.next_restart:\n",
    "            self.batch_since_restart = 0\n",
    "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
    "            self.next_restart += self.cycle_length\n",
    "            self.max_lr *= self.lr_decay\n",
    "            self.best_weights = self.model.get_weights()\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
    "        self.model.set_weights(self.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.applications import VGG19\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "INPUT_SIZE = 256\n",
    "\n",
    "vgg19 = VGG19(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(INPUT_SIZE,INPUT_SIZE,3)\n",
    ")\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(vgg19)\n",
    "model.add(layers.GlobalAveragePooling2D())\n",
    "model.add(BatchNormalization())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))  #softmax\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image preprocessing\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=False,\n",
    "                                   channel_shift_range=10)\n",
    "\n",
    "# featurewise_center=False,\n",
    "# samplewise_center=False, \n",
    "# featurewise_std_normalization=False, \n",
    "# samplewise_std_normalization=False, \n",
    "# zca_whitening=False, \n",
    "# zca_epsilon=1e-06, \n",
    "# rotation_range=0, \n",
    "# width_shift_range=0.0, \n",
    "# height_shift_range=0.0, \n",
    "# brightness_range=None, \n",
    "# shear_range=0.0, \n",
    "# zoom_range=0.0, \n",
    "# channel_shift_range=0.0, \n",
    "# fill_mode='nearest', \n",
    "# cval=0.0, \n",
    "# horizontal_flip=False, \n",
    "# vertical_flip=False, \n",
    "# rescale=None, \n",
    "# preprocessing_function=None, \n",
    "# data_format=None, \n",
    "# validation_split=0.0, \n",
    "# dtype=None\n",
    "\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "INPUT_SIZE = 256\n",
    "batch_size = 16\n",
    "base_dir = \"D:/OCR/id/test_flip\"\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(base_dir+'/Train',\n",
    "                                                 target_size=(INPUT_SIZE, INPUT_SIZE),\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 class_mode='binary') # categorical\n",
    "\n",
    "valid_set = valid_datagen.flow_from_directory(base_dir+'/Dev',\n",
    "                                            target_size=(INPUT_SIZE, INPUT_SIZE),\n",
    "                                            batch_size=batch_size,\n",
    "                                            class_mode='binary')\n",
    "\n",
    "train_num = training_set.samples\n",
    "valid_num = valid_set.samples\n",
    "print(train_num, valid_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#熱重啟參數\n",
    "schedule = SGDRScheduler(min_lr=1e-5,\n",
    "                         max_lr=1e-2,\n",
    "                         steps_per_epoch=np.ceil(train_num/batch_size),\n",
    "                         lr_decay=0.9,\n",
    "                         cycle_length=5,\n",
    "                         mult_factor=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='binary_crossentropy',  # categorical_crossentropy\n",
    "              metrics=['acc']) # categorical_accuracy\n",
    "\n",
    "# early stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)\n",
    "\n",
    "train_num = training_set.samples\n",
    "valid_num = valid_set.samples\n",
    "history0 = model.fit_generator(training_set,\n",
    "                    steps_per_epoch=train_num//batch_size,\n",
    "                    validation_data=valid_set,\n",
    "                    epochs=50,\n",
    "                    validation_steps=valid_num//batch_size,\n",
    "                    callbacks=[schedule, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "acc = history0.history['categorical_accuracy']\n",
    "val_acc = history0.history['val_categorical_accuracy']\n",
    "loss = history0.history['loss']\n",
    "val_loss = history0.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "x=0\n",
    "y=0\n",
    "for i in val_acc[-(len(epochs)//5):]:\n",
    "    x+=1\n",
    "    y+=i\n",
    "print(y/x)\n",
    "\n",
    "#accuracy plot\n",
    "plt.plot(epochs, acc, color='green', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, color='blue', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "#loss plot\n",
    "plt.plot(epochs, loss, color='pink', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, color='red', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
